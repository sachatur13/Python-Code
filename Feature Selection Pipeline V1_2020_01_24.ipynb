{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries used - \n",
    "##### sklearn - feature_selection(VarianceThreshold,SelectFromModel) , model_selection(train_test_split), tree(DecisionTreeClassifier,DecisionTreeRegressor)\n",
    "#####    , linear_model (LinearRegression,LogisticRegression), ensemble(RandomForestClassifier, RandomForestRegressor)\n",
    "#####   , preprocessig(OneHotEncoder)\n",
    "##### scipy - stats\n",
    "##### researchpy - crosstab()\n",
    "##### mlxtend - feature_selection (SequentialFeatureSelector)"
    "#### Updates to be made - Univariate selection method to include more tests, Correlation function to remove the variables from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.feature_selection import VarianceThreshold,SelectFromModel as SFM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import researchpy as rp\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "from sklearn.metrics import roc_auc_score,mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Null values and remove columns with high % of null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_impute_null_features(data):\n",
    "    \n",
    "    ''' \n",
    "    This function takes the dataframe and removes the columns with more than 80%\n",
    "    null values.\n",
    "    Imputes the remaining columns with - \n",
    "    median - when data type is not object\n",
    "    mode - when data type is object\n",
    "    \n",
    "    Parameters - Pandas Dataframe\n",
    "    \n",
    "    Returns - Dataframe without nulls\n",
    "    '''\n",
    "    \n",
    "    numeric_dtypes = ['int','int32','int64','float','float32','float64']\n",
    "    ## Drop columns with more than 80% nulls\n",
    "    data_without_nulls = data.loc[:,data.isnull().mean()<0.8]\n",
    "    \n",
    "    ## Impute Null values\n",
    "    if data_without_nulls.isnull().sum().nunique()>0:\n",
    "        null_columns = data_without_nulls.columns[data_without_nulls.isnull().any()]\n",
    "    \n",
    "    for columns in null_columns:\n",
    "        #print(columns)\n",
    "        if data_without_nulls[columns].dtype == 'O':\n",
    "            data_without_nulls[columns].fillna(data_without_nulls[columns].mode(),inplace = True)\n",
    "        else:\n",
    "            if data_without_nulls[columns].dtype in(numeric_dtypes):\n",
    "                data_without_nulls[columns].fillna(data_without_nulls[columns].median(),inplace = True)\n",
    "      \n",
    "    return data_without_nulls\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_constant_features(data):\n",
    "    \n",
    "    \n",
    "    ''' \n",
    "    This function takes the dataframe and removes the columns with only one value.\n",
    "    \n",
    "    Method used for numeric variables - Variance\n",
    "    Removes the variables where variance is 0.\n",
    "    \n",
    "    Method used for categorical variables - count of unique values\n",
    "    Removes where the sum of the mode of count of unique values is less than 3.\n",
    "    \n",
    "    Parameters - Pandas Dataframe\n",
    "    \n",
    "    Returns - List with the feature names to drop, Dataframe without constant features\n",
    "    '''\n",
    "    print('Removing features with Constant values')\n",
    "    \n",
    "    ## Extracting numeric columns\n",
    "    numeric_dtypes = ['int','int32','int64','float','float32','float64']\n",
    "    data_numeric_columns = data.select_dtypes(include = numeric_dtypes)\n",
    "    numeric_constant_features = []\n",
    "    constant_feature_object_boolean = []\n",
    "    ## Printing numeric columns\n",
    "    \n",
    "    var = VarianceThreshold(threshold=0.0)\n",
    "    var.fit(data_numeric_columns)\n",
    "    \n",
    "    # Printing the constant features\n",
    "    #print('Number of features with constant values : ')\n",
    "    ''''print(len([\n",
    "    x for x in data_numeric_columns.columns\n",
    "    if x not in data_numeric_columns.columns[var.get_support()]\n",
    "    ]))'''\n",
    "\n",
    "    ## Printing vable names with constant values\n",
    "    \n",
    "    for x in data_numeric_columns.columns:\n",
    "        if x not in data_numeric_columns.columns[var.get_support()]:\n",
    "            numeric_constant_features.append(x)\n",
    "    #print('Numeric Features with constant values : ',numeric_constant_features)\n",
    "    \n",
    "    ## Extracting categorical columns\n",
    "    object_types = ['object','bool']\n",
    "    boolean_object_types = data.select_dtypes(include = object_types)\n",
    "    \n",
    "    #print('Number of categorical columns : ',boolean_object_types.shape[1])\n",
    "    \n",
    "    \n",
    "    for i in boolean_object_types.columns:\n",
    "        if boolean_object_types[i].nunique()==1:\n",
    "            constant_feature_object_boolean.append(i)\n",
    "   # print('Categorical and boolean features with constant values : ',constant_feature_object_boolean)\n",
    "    \n",
    "    ## Dropping constant columns\n",
    "    features_to_drop = numeric_constant_features+constant_feature_object_boolean\n",
    "    \n",
    "    data_without_constant_features = data.drop(features_to_drop,axis=1)\n",
    "    \n",
    "    return features_to_drop,data_without_constant_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove features with 99% of same values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_almost_constant(data):\n",
    "    \n",
    "    ''' \n",
    "    This function takes the dataframe and removes the columns with high number of same values.\n",
    "    \n",
    "    Method used for numeric variables - Variance\n",
    "    Removes the variables where variance is 0.\n",
    "    \n",
    "    Method used for categorical variables - count of unique values\n",
    "    Removes where the sum of the mode of count of unique values is less than 3.\n",
    "    \n",
    "    Parameters - Pandas Dataframe\n",
    "    \n",
    "    Returns - List with the feature names to drop, Dataframe without constant features\n",
    "    '''\n",
    "    \n",
    "    print('Removing features with almost constant values') \n",
    "    ## Removing / Imputing Nulls\n",
    "    data = remove_impute_null_features(data)\n",
    "    \n",
    "    ## Extracting numeric columns\n",
    "    numeric_dtypes = ['int','int32','int64','float','float32','float64']\n",
    "    data_numeric_columns = data.select_dtypes(include = numeric_dtypes)\n",
    "    numeric_constant_features = []\n",
    "    constant_feature_object_boolean = []\n",
    "    ## Printing numeric columns\n",
    "   # print('Number of numeric columns: ',data_numeric_columns.shape[1])\n",
    "    \n",
    "    if data_numeric_columns.shape[1]>0:\n",
    "        var = VarianceThreshold(threshold=0.1)\n",
    "        var.fit(data_numeric_columns)\n",
    "    \n",
    "    # Printing the constant features\n",
    "    #    print('Number of features with almost constant values : ')\n",
    "     #   print(len([x for x in data_numeric_columns.columns if x not in data_numeric_columns.columns[var.get_support()]]))\n",
    "\n",
    "    ## Printing varaible names with constant values\n",
    "        \n",
    "        for x in data_numeric_columns.columns:\n",
    "            if x not in data_numeric_columns.columns[var.get_support()]:\n",
    "                numeric_constant_features.append(x)\n",
    "        \n",
    "    ## Extracting categorical columns\n",
    "    object_types = ['object','bool']\n",
    "    boolean_object_types = data.select_dtypes(include = object_types)\n",
    "    \n",
    "    #print('Number of categorical columns : ',boolean_object_types.shape[1])\n",
    "   \n",
    "    \n",
    "    for i in boolean_object_types.columns:\n",
    "        if (((boolean_object_types[i].value_counts())/(data.shape[0])).sort_values(ascending = False))[0]>=0.9:\n",
    "            #print(i,boolean_object_types[i].mode().value_counts().sum())\n",
    "            constant_feature_object_boolean.append(i)\n",
    "            \n",
    "    ## Dropping constant columns\n",
    "    features_to_drop = numeric_constant_features+constant_feature_object_boolean\n",
    "    \n",
    "    data_without_constant_features = data.drop(features_to_drop,axis=1)\n",
    "    \n",
    "    return features_to_drop,data_without_constant_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_columns(data,thresh):\n",
    "    \n",
    "    ''' \n",
    "    This function takes the dataframe and returns the correlated columns.\n",
    "    \n",
    "    Method used for numeric variables - pearson correlation\n",
    "       \n",
    "    Method used for categorical variables - chi2 test of independence\n",
    "    alpha value - 0.05\n",
    "        \n",
    "    Parameters - Pandas Dataframe, Threshold for correlation\n",
    "    \n",
    "    Returns - List with the correlated feature names to drop, Dataframe with correlated features p value and chi2 values\n",
    "    '''\n",
    "     \n",
    "    print('Running correlation check using pearson correlation and chi2 statistic')\n",
    "    ## Remove / Imputing Nulls\n",
    "    data = remove_impute_null_features(data)\n",
    "    \n",
    "    ## Extracting numeric columns\n",
    "    numeric_dtypes = ['int','int32','int64','float','float32','float64']\n",
    "    data_numeric_columns = data.select_dtypes(include = numeric_dtypes)\n",
    "   \n",
    "    numeric_correlated_features = set()\n",
    "    \n",
    "    if(data_numeric_columns.shape[1]>0): \n",
    "        correlation_matrix = data_numeric_columns.corr()\n",
    "      \n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if (abs(correlation_matrix.iloc[i,j])>thresh) and (abs(correlation_matrix.iloc[i,j]!=1)):\n",
    "                    column_name = correlation_matrix.columns[i]\n",
    "                    numeric_correlated_features.add(column_name)\n",
    "    \n",
    "    ## Extracting categorical columns\n",
    "    object_types = ['object','bool']\n",
    "    boolean_object_types = data.select_dtypes(include = object_types)\n",
    "    \n",
    "    categorical_to_drop = set()\n",
    "    column_1 = []\n",
    "    column_2 = []\n",
    "    chi2_value = []\n",
    "    p_value = []\n",
    "    \n",
    "    if(boolean_object_types.shape[1]>0):\n",
    "        for i in boolean_object_types.columns:\n",
    "            #print(i)\n",
    "            for j in boolean_object_types.columns.drop(i):\n",
    "                #print(j)\n",
    "                cross_table,results = rp.crosstab(boolean_object_types[i],boolean_object_types[j],prop='col',test = 'chi-square')\n",
    "                #print(cross_table,results)\n",
    "                p = results.iloc[1,1]\n",
    "                chi2 = results.iloc[0,1]\n",
    "                ## Running chi2 on the columns\n",
    "                #chi2, p = stats.chi2_contingency(cross_table)\n",
    "                #print('Column 1 : ',i)\n",
    "                #print('Column 2 : ',j)\n",
    "                #print('Chi square value: ',chi2)\n",
    "                #print('P value: ',p_value)\n",
    "                column_1.append(i)\n",
    "                column_2.append(j)\n",
    "                chi2_value.append(chi2)\n",
    "                p_value.append(p)\n",
    "                \n",
    "                if(p>0.05):\n",
    "                    categorical_to_drop.add(j)\n",
    "    \n",
    "    correlated_features_to_drop = numeric_correlated_features.union(categorical_to_drop)\n",
    "    #chi_2_values_DF = list(zip(column_1,column_2,chi2,p_value))\n",
    "    chi2_values_dataframe = pd.DataFrame({'Column 1':column_1,\n",
    "                                          'Column 2':column_2,\n",
    "                                          'Chi2 Value':chi2_value,\n",
    "                                          'P Value':p_value})\n",
    "    \n",
    "    return correlated_features_to_drop,chi2_values_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Model metrics for univariate feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_model_metric(data,variable_to_predict,problem_type,thresh):\n",
    "    \n",
    "    ''' \n",
    "    This function takes the dataframe and removes the features based on univariate model metric.\n",
    "    It runs DecisionTree Classifier and Regressor models using one feature against the target variable and \n",
    "    computes the metric. Then returns the variables npt meeting the threshold value.\n",
    "        \n",
    "    Metric for regression problem - mean squared error.\n",
    "    Metric for classification problem - roc auc value.\n",
    "    \n",
    "    Parameters - \n",
    "    Pandas Dataframe\n",
    "    Variable to Predict - Target Variable\n",
    "    Problem Type - classfication or regression\n",
    "    thresh - Threshold value for the metric to select variables.\n",
    "    \n",
    "    Returns - \n",
    "    features_not_meeting_thresh - List with feature names not meeting the threshold\n",
    "    metric - All variable names with the metric values\n",
    "    '''\n",
    "    \n",
    "    print('Running univariate selection using roc_auc and mse')\n",
    "    numeric_dtypes = ['int','int32','int64','float','float32','float64']\n",
    "\n",
    "    data_without_nulls = remove_impute_null_features(data)\n",
    "    \n",
    "    X, y = data_without_nulls.drop(variable_to_predict,axis = 1),data_without_nulls[variable_to_predict]\n",
    "    \n",
    "    ## Label Encoding\n",
    "    \n",
    "    data_column_names = data_without_nulls.columns    \n",
    "    columns_to_dummy = X.select_dtypes(include = 'O').columns\n",
    "    \n",
    "    encoded_data = pd.get_dummies(X,columns = columns_to_dummy,prefix_sep = '**')\n",
    "    \n",
    "    ## Train test split\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 42,shuffle = True)\n",
    "    \n",
    "    features_not_meeting_thresh = []\n",
    "    metric = []\n",
    "    features = []\n",
    "    ## If Classification problem , use ROC AUC value\n",
    "    if problem_type == 'classification':\n",
    "        print('Using ROC AUC for selecting features')\n",
    "        \n",
    "        \n",
    "        for x in X_train.columns:\n",
    "            classif = DecisionTreeClassifier()\n",
    "            #print(x)\n",
    "            if X_train[x].dtype == 'O':\n",
    "                encoded_training_data = pd.get_dummies(X_train[x],prefix_sep = '**')\n",
    "                encoded_test_data = pd.get_dummies(X_test[x],prefix_sep = '**')\n",
    "                \n",
    "                missing_cols_test = set(encoded_training_data.columns) - set(encoded_test_data.columns)\n",
    "                missing_cols_train = set(encoded_test_data.columns) - set(encoded_training_data.columns)\n",
    "                \n",
    "                for col_test in missing_cols_test:\n",
    "                    encoded_test_data[col_test] = 0\n",
    "                \n",
    "                for col_train in missing_cols_train:\n",
    "                    encoded_training_data[col_train] = 0\n",
    "                \n",
    "                encoded_test_data = encoded_test_data[encoded_training_data.columns]\n",
    "                #One_hot_encoding.fit_transform(X_train[x].to_frame())\n",
    "                #encoded_test_data = One_hot_encoding.fit_transform(X_test[x].to_frame())\n",
    "              \n",
    "                classif.fit(encoded_training_data,y_train)\n",
    "                predicted_score = classif.predict_proba(encoded_test_data)            \n",
    "                roc_score = roc_auc_score(y_test,predicted_score[:,1])\n",
    "                features.append(x)\n",
    "                metric.append(roc_auc_score(y_test,predicted_score[:,1]))\n",
    "            else:\n",
    "                \n",
    "                classif.fit(X_train[x].to_frame(),y_train)\n",
    "                predicted_score = classif.predict_proba(X_test[x].to_frame())            \n",
    "                roc_score = roc_auc_score(y_test,predicted_score[:,1])\n",
    "                features.append(x)\n",
    "                metric.append(roc_auc_score(y_test,predicted_score[:,1]))\n",
    "                \n",
    "            if roc_score<=thresh:\n",
    "                features_not_meeting_thresh.append(x)         \n",
    "            \n",
    "        metric = pd.Series(metric)\n",
    "        metric.index = X_train.columns\n",
    "        \n",
    "    if problem_type =='regression':\n",
    "        print('Using MSE for selecting features')\n",
    "        \n",
    "                \n",
    "        for x in X_train.columns:\n",
    "            #print(x)\n",
    "            reg = DecisionTreeRegressor()\n",
    "            if X_train[x].dtype == 'O':\n",
    "                encoded_training_data = pd.get_dummies(X_train[x],prefix_sep = '**')\n",
    "                encoded_test_data = pd.get_dummies(X_test[x],prefix_sep = '*')\n",
    "                \n",
    "                \n",
    "                missing_cols_test = set(encoded_training_data.columns) - set(encoded_test_data.columns)\n",
    "                missing_cols_train = set(encoded_test_data.columns) - set(encoded_training_data.columns)\n",
    "                \n",
    "                for col_test in missing_cols_test:\n",
    "                    encoded_test_data[col_test] = 0\n",
    "                \n",
    "                for col_train in missing_cols_train:\n",
    "                    encoded_training_data[col_train] = 0\n",
    "                #encoded_training_data = One_hot_encoding.fit_transform(X_train[x].to_frame())\n",
    "                #encoded_test_data = One_hot_encoding.fit_transform(X_test[x].to_frame())\n",
    "                reg.fit(encoded_training_data,y_train)\n",
    "                #print(encoded_training_data.columns)\n",
    "                #print(encoded_test_data.columns)\n",
    "                predicted_value = reg.predict(encoded_test_data)            \n",
    "                mse = mean_squared_error(y_test,predicted_value)\n",
    "                metric.append(mse)              \n",
    "                \n",
    "\n",
    "            else:\n",
    "                \n",
    "                reg.fit(X_train[x].to_frame(),y_train)\n",
    "                predicted_value = reg.predict(X_test[x].to_frame())            \n",
    "                mse = mean_squared_error(y_test,predicted_value)\n",
    "                metric.append(mse)\n",
    "            \n",
    "            if mse>=thresh:\n",
    "                features_not_meeting_thresh.append(x)\n",
    "        \n",
    "        metric = pd.Series(metric)\n",
    "        metric.index = X_train.columns\n",
    "        metric = metric.to_frame()\n",
    "    \n",
    "    \n",
    "    return features_not_meeting_thresh,metric\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Forward / Backward Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_feature_selection(data,model,variable_to_predict,problem_type,num_features,direction):\n",
    "    \n",
    "    ''' \n",
    "    This function takes the dataframe and removes the features based on forward or backward feature selection techniques.\n",
    "    It runs the following models based on the model and the problem type.\n",
    "    Model - Linear \n",
    "        Problem Type - classification\n",
    "            ML Algorithm - Logistic Regression\n",
    "        \n",
    "        Problem Type - regression\n",
    "            ML Algorithm - Linear Regression\n",
    "   \n",
    "    Model - tree\n",
    "        Problem Type - classification\n",
    "            ML Algorithm - Decision Tree Classifier\n",
    "        \n",
    "        Problem Type - regression\n",
    "            ML Algorithm - Decision Tree Regressor\n",
    "            \n",
    "    Model - ensemble \n",
    "        Problem Type - classification\n",
    "            ML Algorithm - Random Forest Classifier\n",
    "        \n",
    "        Problem Type - regression\n",
    "            ML Algorithm - Random Forest Regressor\n",
    "\n",
    "    Evaluation metric - \n",
    "        Classification - roc_auc, regression - mse\n",
    "    \n",
    "    Parameters - \n",
    "    data - Pandas Dataframe\n",
    "    model - linear, tree, ensemble\n",
    "    Variable to Predict - Target Variable\n",
    "    Problem Type - classfication or regression\n",
    "    num_features - number of features to return\n",
    "    direction - True (Forward), False (Backward)\n",
    "    \n",
    "    Returns - \n",
    "    selected_feature_names_returned - List with best selected feature names.\n",
    "    \n",
    "    '''\n",
    "    print('Running stepwise feature selection')\n",
    "    ## Remove Nulls\n",
    "    data_without_nulls = remove_impute_null_features(data)\n",
    "    X, y = data_without_nulls.drop(variable_to_predict,axis = 1),data_without_nulls[variable_to_predict]\n",
    "    ## Label Encoding\n",
    "    One_hot_encoding = OneHotEncoder()\n",
    "    \n",
    "    data_column_names = data_without_nulls.columns    \n",
    "    columns_to_dummy = data_without_nulls.select_dtypes(include = 'O').columns\n",
    "    \n",
    "    encoded_data = pd.get_dummies(X,columns = columns_to_dummy,prefix_sep = '**')\n",
    "    \n",
    "    if model == 'linear':\n",
    "        from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "        \n",
    "        if problem_type == 'classification':\n",
    "            logistic = LogisticRegression()\n",
    "            \n",
    "            sfs_classification = sfs(logistic,\n",
    "                                    k_features=num_features,\n",
    "                                    forward = direction,\n",
    "                                    scoring = 'roc_auc',\n",
    "                                    cv = 5)\n",
    "            sfs_classification.fit(encoded_data,y)\n",
    "            selected_feature_names = sfs_classification.k_feature_names_\n",
    "            \n",
    "        \n",
    "        if problem_type == 'regression':\n",
    "            linear = LinearRegression()\n",
    "            \n",
    "            sfs_regression = sfs(linear,\n",
    "                                    k_features=num_features,\n",
    "                                    forward = direction,\n",
    "                                    scoring = 'neg_mean_squared_error',\n",
    "                                    cv = 5)\n",
    "            sfs_regression.fit(encoded_data,y)\n",
    "            selected_feature_names = sfs_regression.k_feature_names_\n",
    "        \n",
    "    if model == 'tree':\n",
    "        from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "        \n",
    "        if problem_type == 'classification':\n",
    "            d_classifier = DecisionTreeClassifier()\n",
    "            \n",
    "            sfs_classification = sfs(d_classifier,\n",
    "                                    k_features=num_features,\n",
    "                                    forward = direction,\n",
    "                                    scoring = 'roc_auc',\n",
    "                                    cv = 5)\n",
    "            sfs_classification.fit(encoded_data,y)\n",
    "            selected_feature_names = sfs_classification.k_feature_names_\n",
    "        \n",
    "        if problem_type == 'regression':\n",
    "            d_regression = DecisionTreeRegressor()\n",
    "            \n",
    "            sfs_regression = sfs(d_regression,\n",
    "                                    k_features=num_features,\n",
    "                                    forward = direction,\n",
    "                                    scoring = 'neg_mean_squared_error',\n",
    "                                    cv = 5)\n",
    "            sfs_regression.fit(encoded_data,y)\n",
    "            selected_feature_names = sfs_regression.k_feature_names_\n",
    "            \n",
    "    if model == 'ensemble':\n",
    "        from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "        \n",
    "        \n",
    "        if problem_type == 'classification':\n",
    "            r_classifier = RandomForestClassifier()\n",
    "            \n",
    "            sfs_classification = sfs(r_classifier,\n",
    "                                    k_features=num_features,\n",
    "                                    forward = direction,\n",
    "                                    scoring = 'roc_auc',\n",
    "                                    cv = 5)\n",
    "            sfs_classification.fit(encoded_data,y)\n",
    "            selected_feature_names = sfs_classification.k_feature_names_\n",
    "        \n",
    "        if problem_type == 'regression':\n",
    "            r_regression = RandomForestRegressor()\n",
    "            \n",
    "            sfs_regression = sfs(r_regression,\n",
    "                                    k_features=num_features,\n",
    "                                    forward = direction,\n",
    "                                    scoring = 'neg_mean_squared_error',\n",
    "                                    cv = 5)\n",
    "            sfs_regression.fit(encoded_data,y)\n",
    "            selected_feature_names = sfs_regression.k_feature_names_\n",
    "    \n",
    "    selected = list(selected_feature_names)\n",
    "    selected_feature_names_returned = list(set([i.split('**',1)[0] for i in selected]))\n",
    "    \n",
    "    return selected_feature_names_returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using Random Forest Feature importance and variable coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_from_feat_importance_coeff(data,model,variable_to_predict,problem_type,num_features):\n",
    "    \n",
    "    ''' \n",
    "    This function takes the dataframe and removes the features based on random forest feature importance and coefficient values\n",
    "    greater than or equal to the mean of the values.\n",
    "    It runs the following models based on the model and the problem type.\n",
    "    Model - Linear \n",
    "        Problem Type - classification\n",
    "            ML Algorithm - Logistic Regression\n",
    "        \n",
    "        Problem Type - regression\n",
    "            ML Algorithm - Linear Regression\n",
    "   \n",
    "    Model - ensemble \n",
    "        Problem Type - classification\n",
    "            ML Algorithm - Random Forest Classifier\n",
    "        \n",
    "        Problem Type - regression\n",
    "            ML Algorithm - Random Forest Regressor\n",
    "\n",
    "    Parameters - \n",
    "    data - Pandas Dataframe\n",
    "    model - linear, ensemble\n",
    "    Variable to Predict - Target Variable\n",
    "    Problem Type - classfication or regression\n",
    "    num_features - number of features to return\n",
    "    \n",
    "    Returns - \n",
    "    selected_feature_names_returned - List with best selected feature names.\n",
    "    \n",
    "    '''\n",
    "    print('Running feature selection using feature importance and coefficient values')\n",
    "    ## Remove Nulls\n",
    "    data_without_nulls = remove_impute_null_features(data)\n",
    "    X, y = data_without_nulls.drop(variable_to_predict,axis = 1),data_without_nulls[variable_to_predict]\n",
    "    \n",
    "    data_column_names = data_without_nulls.columns    \n",
    "    columns_to_dummy = data_without_nulls.select_dtypes(include = 'O').columns\n",
    "    \n",
    "    encoded_data = pd.get_dummies(X,columns = columns_to_dummy,prefix_sep = '**')\n",
    "    \n",
    "    if model == 'linear':\n",
    "        from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "        \n",
    "        if problem_type == 'classification':\n",
    "            select_from_logistic = SFM(LogisticRegression(),threshold='mean',max_features = num_features)\n",
    "            \n",
    "            select_from_logistic.fit(encoded_data,y)\n",
    "            selected_feature_names = encoded_data.columns[select_from_logistic.get_support()]\n",
    "            \n",
    "        if problem_type == 'regression':\n",
    "                        \n",
    "            select_from_linear = SFM(LinearRegression(),threshold='mean',max_features = num_features)\n",
    "            \n",
    "            select_from_linear.fit(encoded_data,y)\n",
    "            selected_feature_names = encoded_data.columns[select_from_linear.get_support()]           \n",
    "    \n",
    "    if model == 'ensemble':\n",
    "        from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "        \n",
    "        \n",
    "        if problem_type == 'classification':\n",
    "            r_classifier = RandomForestClassifier()\n",
    "            \n",
    "            select_from_random_classifier = SFM(RandomForestClassifier(),threshold='mean',max_features = num_features)\n",
    "            \n",
    "            select_from_random_classifier.fit(encoded_data,y)\n",
    "            selected_feature_names = encoded_data.columns[select_from_random_classifier.get_support()]           \n",
    "        \n",
    "        if problem_type == 'regression':\n",
    "            select_from_random_regressor = SFM(RandomForestRegressor(),threshold='mean',max_features = num_features)\n",
    "            \n",
    "            select_from_random_regressor.fit(encoded_data,y)\n",
    "            selected_feature_names = encoded_data.columns[select_from_random_regressor.get_support()]           \n",
    "        \n",
    "    selected = list(selected_feature_names)\n",
    "    selected_feature_names_returned = list(set([i.split('**',1)[0] for i in selected]))\n",
    "        \n",
    "    return selected_feature_names_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_pipeline(data,model,variable_to_predict,problem_type,num_features,direction,thresh):\n",
    "\n",
    "    \n",
    "    \n",
    "    imputed_data = remove_impute_null_features(data);\n",
    "\n",
    "    print('#########################################')\n",
    "    features_with_constant_values, data_without_constants = remove_constant_features(imputed_data);\n",
    "\n",
    "    print('#########################################')\n",
    "    quasi_constant_features, data_without_quasi_constants = remove_almost_constant(data_without_constants);\n",
    "\n",
    "    print('#########################################')\n",
    "    correlated_features, chi2_values_dataframe = remove_correlated_columns(data_without_quasi_constants,thresh);\n",
    "\n",
    "    print('#########################################')\n",
    "    features_not_meeting_threshold, feature_metrics = univariate_model_metric(data_without_quasi_constants,variable_to_predict, problem_type, thresh);\n",
    "\n",
    "    print('#########################################')\n",
    "    stepwise_selected_features = stepwise_feature_selection(data_without_quasi_constants,model,variable_to_predict,problem_type,num_features,direction);\n",
    "\n",
    "    print('#########################################')\n",
    "    feature_importance_selected_features = select_from_feat_importance_coeff(data_without_quasi_constants,model,variable_to_predict,problem_type,num_features)\n",
    "\n",
    "    print('#########################################')\n",
    "    print('Feature Selection Complete, here are the selected features from every step')\n",
    "    print('#########################################')\n",
    "\n",
    "    print('\\n')\n",
    "    print('Features with constant values: ')\n",
    "    print('#######')\n",
    "    print(features_with_constant_values)\n",
    "\n",
    "    print('\\n')\n",
    "    print('Features with almost constant values: ')\n",
    "    print('#######')\n",
    "    print(quasi_constant_features)\n",
    "\n",
    "    print('\\n')\n",
    "    print('Features with correlation: ')\n",
    "    print('#######')\n",
    "    print(correlated_features)\n",
    "    print(chi2_values_dataframe)\n",
    "\n",
    "    print('\\n')\n",
    "    print('Features from univariate test: ')\n",
    "    print('#######')\n",
    "    print(features_not_meeting_threshold)\n",
    "\n",
    "    print('\\n')\n",
    "    print('Features from stepwise selection: ')\n",
    "    print('#######')\n",
    "    print(stepwise_selected_features)\n",
    "\n",
    "    print('\\n')\n",
    "    print('Features from feature importance: ')\n",
    "    print('#######')\n",
    "    print(feature_importance_selected_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 80 columns):\n",
      "MSSubClass       1460 non-null int64\n",
      "MSZoning         1460 non-null object\n",
      "LotFrontage      1201 non-null float64\n",
      "LotArea          1460 non-null int64\n",
      "Street           1460 non-null object\n",
      "Alley            91 non-null object\n",
      "LotShape         1460 non-null object\n",
      "LandContour      1460 non-null object\n",
      "Utilities        1460 non-null object\n",
      "LotConfig        1460 non-null object\n",
      "LandSlope        1460 non-null object\n",
      "Neighborhood     1460 non-null object\n",
      "Condition1       1460 non-null object\n",
      "Condition2       1460 non-null object\n",
      "BldgType         1460 non-null object\n",
      "HouseStyle       1460 non-null object\n",
      "OverallQual      1460 non-null int64\n",
      "OverallCond      1460 non-null int64\n",
      "YearBuilt        1460 non-null int64\n",
      "YearRemodAdd     1460 non-null int64\n",
      "RoofStyle        1460 non-null object\n",
      "RoofMatl         1460 non-null object\n",
      "Exterior1st      1460 non-null object\n",
      "Exterior2nd      1460 non-null object\n",
      "MasVnrType       1452 non-null object\n",
      "MasVnrArea       1452 non-null float64\n",
      "ExterQual        1460 non-null object\n",
      "ExterCond        1460 non-null object\n",
      "Foundation       1460 non-null object\n",
      "BsmtQual         1423 non-null object\n",
      "BsmtCond         1423 non-null object\n",
      "BsmtExposure     1422 non-null object\n",
      "BsmtFinType1     1423 non-null object\n",
      "BsmtFinSF1       1460 non-null int64\n",
      "BsmtFinType2     1422 non-null object\n",
      "BsmtFinSF2       1460 non-null int64\n",
      "BsmtUnfSF        1460 non-null int64\n",
      "TotalBsmtSF      1460 non-null int64\n",
      "Heating          1460 non-null object\n",
      "HeatingQC        1460 non-null object\n",
      "CentralAir       1460 non-null object\n",
      "Electrical       1459 non-null object\n",
      "1stFlrSF         1460 non-null int64\n",
      "2ndFlrSF         1460 non-null int64\n",
      "LowQualFinSF     1460 non-null int64\n",
      "GrLivArea        1460 non-null int64\n",
      "BsmtFullBath     1460 non-null int64\n",
      "BsmtHalfBath     1460 non-null int64\n",
      "FullBath         1460 non-null int64\n",
      "HalfBath         1460 non-null int64\n",
      "BedroomAbvGr     1460 non-null int64\n",
      "KitchenAbvGr     1460 non-null int64\n",
      "KitchenQual      1460 non-null object\n",
      "TotRmsAbvGrd     1460 non-null int64\n",
      "Functional       1460 non-null object\n",
      "Fireplaces       1460 non-null int64\n",
      "FireplaceQu      770 non-null object\n",
      "GarageType       1379 non-null object\n",
      "GarageYrBlt      1379 non-null float64\n",
      "GarageFinish     1379 non-null object\n",
      "GarageCars       1460 non-null int64\n",
      "GarageArea       1460 non-null int64\n",
      "GarageQual       1379 non-null object\n",
      "GarageCond       1379 non-null object\n",
      "PavedDrive       1460 non-null object\n",
      "WoodDeckSF       1460 non-null int64\n",
      "OpenPorchSF      1460 non-null int64\n",
      "EnclosedPorch    1460 non-null int64\n",
      "3SsnPorch        1460 non-null int64\n",
      "ScreenPorch      1460 non-null int64\n",
      "PoolArea         1460 non-null int64\n",
      "PoolQC           7 non-null object\n",
      "Fence            281 non-null object\n",
      "MiscFeature      54 non-null object\n",
      "MiscVal          1460 non-null int64\n",
      "MoSold           1460 non-null int64\n",
      "YrSold           1460 non-null int64\n",
      "SaleType         1460 non-null object\n",
      "SaleCondition    1460 non-null object\n",
      "SalePrice        1460 non-null int64\n",
      "dtypes: float64(3), int64(34), object(43)\n",
      "memory usage: 912.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('C:\\\\Users\\\\saket\\\\Desktop\\\\Feature Selection for ML\\\\train.csv')\n",
    "\n",
    "data.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################\n",
      "Removing features with Constant values\n",
      "#########################################\n",
      "Removing features with almost constant values\n",
      "#########################################\n",
      "Running correlation check using pearson correlation and chi2 statistic\n"
     ]
    }
   ],
   "source": [
    "feature_selection_pipeline(data,'linear','SalePrice','regression',5,True,0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_col = ['GarageArea', 'MasVnrType', 'ExterCond', 'MSZoning', 'KitchenQual', 'FireplaceQu', 'LotConfig', 'GrLivArea', 'LandContour', 'BldgType', 'GarageQual', 'SaleType', 'SalePrice', 'BsmtFullBath', 'Foundation', 'GarageFinish', 'FullBath', 'HeatingQC', 'HalfBath', 'LotShape', 'GarageYrBlt', 'TotRmsAbvGrd', 'RoofStyle', 'BsmtExposure', '1stFlrSF', 'GarageCars', 'HouseStyle', 'SaleCondition', 'ExterQual', 'Exterior1st', 'Condition1', 'BsmtFinType1', 'BsmtFinType2', 'BsmtCond']\n",
    "data_y = ['SalePrice']\n",
    "X = data[data_col]\n",
    "y = data[data_y]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lg = LinearRegression()\n",
    "X_ = remove_impute_null_features(X)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_,y)\n",
    "\n",
    "data_dummy = pd.get_dummies(X_train)\n",
    "data_dumm_T = pd.get_dummies(X_test)\n",
    "\n",
    "missing_cols_test = set(data_dummy.columns) - set(data_dumm_T.columns)\n",
    "missing_cols_train = set(data_dumm_T.columns) - set(data_dummy.columns)\n",
    "for col_test in missing_cols_test:\n",
    "    data_dumm_T[col_test] = 0\n",
    "for col_train in missing_cols_train:\n",
    "    data_dummy[col_train] = 0\n",
    "                \n",
    "lg.fit(data_dummy,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lg.predict(data_dumm_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0902648398134547e-20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(data_dumm_T,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
